{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>descr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, descr]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('corpus.csv')\n",
    "df = df2.drop(columns=['story', 'url'])\n",
    "df = df.drop_duplicates()\n",
    "df.loc[lambda df: df['title']==\"Video Eurosport\"]\n",
    "df = df.drop([1308])\n",
    "df.loc[lambda df: df['title']==\"Video Eurosport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nkanatov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def tokenize(string):\n",
    "    string = re.sub(r'[^\\w\\s]', ' ',  string)\n",
    "    tokens = string.lower().split()\n",
    "    words = [t for t in tokens if len(t) > 2]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195616, 16759)\n",
      "16759\n",
      "(97789, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>descr</th>\n",
       "      <th>title_idx</th>\n",
       "      <th>descr_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Русские биатлонистки покорили спринт на Универ...</td>\n",
       "      <td>Это был не чемпионат России.</td>\n",
       "      <td>[1291, 4430, 7359, 9677, 12397, 13641, 14960]</td>\n",
       "      <td>[12278, 15983, 16591]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Россия обыграла США на Универсиаде и возглавил...</td>\n",
       "      <td>Красивая победа «Красной машины».</td>\n",
       "      <td>[2293, 3316, 8768, 12293, 14087, 14960]</td>\n",
       "      <td>[6205, 6224, 7261, 9955]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>«Аякс» не проигрывает, если открывает счет. 6 ...</td>\n",
       "      <td>Выдающаяся статистика сказочного матча.</td>\n",
       "      <td>[1002, 9234, 11353, 11700, 11952, 14076, 15181]</td>\n",
       "      <td>[7227, 13795]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Krone: в Австрии арестовали лыжника Дюрра. Он ...</td>\n",
       "      <td>Попался сам – пусть попадутся и другие.</td>\n",
       "      <td>[410, 4015, 6842, 10492]</td>\n",
       "      <td>[4111, 10540, 11554]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Французская конькобежка подрезала кореянку, и ...</td>\n",
       "      <td>Женский финал на универсиаде в Красноярске пол...</td>\n",
       "      <td>[2251, 6081, 12033, 15508]</td>\n",
       "      <td>[4321, 10454, 14960, 15319]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Русские биатлонистки покорили спринт на Универ...   \n",
       "1  Россия обыграла США на Универсиаде и возглавил...   \n",
       "2  «Аякс» не проигрывает, если открывает счет. 6 ...   \n",
       "4  Krone: в Австрии арестовали лыжника Дюрра. Он ...   \n",
       "5  Французская конькобежка подрезала кореянку, и ...   \n",
       "\n",
       "                                               descr  \\\n",
       "0                       Это был не чемпионат России.   \n",
       "1                  Красивая победа «Красной машины».   \n",
       "2            Выдающаяся статистика сказочного матча.   \n",
       "4            Попался сам – пусть попадутся и другие.   \n",
       "5  Женский финал на универсиаде в Красноярске пол...   \n",
       "\n",
       "                                         title_idx  \\\n",
       "0    [1291, 4430, 7359, 9677, 12397, 13641, 14960]   \n",
       "1          [2293, 3316, 8768, 12293, 14087, 14960]   \n",
       "2  [1002, 9234, 11353, 11700, 11952, 14076, 15181]   \n",
       "4                         [410, 4015, 6842, 10492]   \n",
       "5                       [2251, 6081, 12033, 15508]   \n",
       "\n",
       "                     descr_idx  \n",
       "0        [12278, 15983, 16591]  \n",
       "1     [6205, 6224, 7261, 9955]  \n",
       "2                [7227, 13795]  \n",
       "4         [4111, 10540, 11554]  \n",
       "5  [4321, 10454, 14960, 15319]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. токенизируем тексты с опциональной лемматизацией\n",
    "# 3. строим словарь из слов, которые встречаются достаточно часто\n",
    "# 4. представляем каждый текст как набор индексов слов в словаре\n",
    "\n",
    "corpus = pd.concat([df['title'], df['descr']])\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(\n",
    "    min_df=10,\n",
    "    max_df = 0.8,\n",
    "    stop_words = stopwords.words('russian'),\n",
    "    tokenizer=tokenize)\n",
    "\n",
    "transformed_corpus = vectorizer.fit_transform(corpus)\n",
    "print(transformed_corpus.shape)\n",
    "\n",
    "vocab = vectorizer.vocabulary_\n",
    "num_words = len(vocab)\n",
    "print(num_words)\n",
    "\n",
    "titles = vectorizer.transform(df['title'])\n",
    "descrs = vectorizer.transform(df['descr'])\n",
    "df['title_idx'] = [i.indices for i in titles]\n",
    "df['descr_idx'] = [i.indices for i in descrs]\n",
    "df=df[df['title_idx'].map(len) !=0]\n",
    "df=df[df['descr_idx'].map(len) !=0]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. инициализируем для каждого слова случайный вектор некоторой длины\n",
    "matrix_size=512\n",
    "def initialize():\n",
    "    matrix = np.array(np.random.normal(0.0001, 0.00001, matrix_size * num_words))\n",
    "    matrix = np.reshape(matrix, newshape=(num_words, matrix_size))\n",
    "    vt = np.zeros(matrix.shape, dtype=np.float16)\n",
    "    return matrix, vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7. представляем каждый текст как результат функции агрегации avg, применённой к векторам составляющих его слов\n",
    "\n",
    "def get_vector(text_idx):\n",
    "    return np.mean(matrix[text_idx], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 8. считаем функцию потерь warp\n",
    "\n",
    "def warp(data, first_title_vector, first_descr_vector, first_index, gamma = 1.0):\n",
    "\n",
    "    max_num_trials = 100\n",
    "    right_scalar = np.dot(first_title_vector, first_descr_vector)\n",
    "    sample_score_margin = -1\n",
    "    num_trials = 0\n",
    "    used_description_idx = set([first_index])\n",
    "\n",
    "    while ((sample_score_margin <= 0) and (num_trials < max_num_trials)):\n",
    "\n",
    "        # randomly sample a negative label and get its aggregated vector\n",
    "        random_index = np.random.randint(len(data))\n",
    "        while (random_index in used_description_idx):\n",
    "            random_index = np.random.randint(len(data))\n",
    "\n",
    "        random_descr = data.iloc[[random_index]]['descr_idx'].values[0]\n",
    "        used_description_idx.add(random_index)\n",
    "        random_descr_vector = get_vector(random_descr)\n",
    "\n",
    "        wrong_scalar = np.dot(first_title_vector, random_descr_vector)\n",
    "\n",
    "        num_trials += 1\n",
    "        # calculate the score margin \n",
    "        sample_score_margin = 1 - right_scalar + wrong_scalar\n",
    "\n",
    "        if sample_score_margin > 0:\n",
    "            loss_weight = sum([1/j for j in range(1, num_trials + 1)])\n",
    "            loss = loss_weight * (gamma - right_scalar + wrong_scalar)\n",
    "            if (loss > 0):\n",
    "                return random_descr, (random_descr_vector - first_descr_vector, (-1) * first_title_vector, first_title_vector)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 9. обновляем параметры модели\n",
    "\n",
    "def update_sgdm(idx, gradient, vt, gamma=0.9, learning_rate=0.01):\n",
    "    vt[idx] = gamma * vt[idx] + learning_rate * gradient\n",
    "    matrix[idx] -= vt[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10. убедиться что параметры модели удовлетворяют ограничениям unitnorm\n",
    "\n",
    "def unitnorm(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        norm = np.sqrt(np.sum(matrix[i] ** 2))\n",
    "        if (round(norm, 3) != 1):\n",
    "            matrix[i] /= norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# метрика качества модели\n",
    "\n",
    "def get_dist(v1, v2):\n",
    "    return 1 - np.dot(v1, np.transpose(v2)) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "  \n",
    "def recall(data, k = 10):\n",
    "    n_test = len(data)\n",
    "    recall = 0\n",
    "    descr_emb = [get_vector(data.iloc[[i]]['descr_idx'].values[0]) for i in range(n_test)]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        title_emb = get_vector(data.iloc[[i]]['title_idx'].values[0])     \n",
    "        dist_arr = []\n",
    "        for i_1 in range(n_test):\n",
    "            dist_arr.append(get_dist(title_emb, descr_emb[i_1]))\n",
    "\n",
    "        sort_dist_arr = np.sort(dist_arr)\n",
    "        if dist_arr[i] <= sort_dist_arr[k-1]:\n",
    "            recall += 1\n",
    "\n",
    "    return recall / n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96808 981\n"
     ]
    }
   ],
   "source": [
    "train_data = df[:96808]\n",
    "test_data = df[96808:97808]\n",
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "def train(train_data, test_data, n_epochs, vt, gamma, alpha, check_step=1000):\n",
    "    for epoch in range(n_epochs):\n",
    "        t1 = time.process_time()\n",
    "        seq = np.random.permutation(len(train_data))\n",
    "        n_steps = 0\n",
    "        for i in seq:\n",
    "            sample = train_data.iloc[[i]]\n",
    "            title = sample['title_idx'].values[0]\n",
    "            descr = sample['descr_idx'].values[0]\n",
    "            first_title_vector = get_vector(title)\n",
    "            first_descr_vector = get_vector(descr)    \n",
    "            output_warp = warp(data, first_title_vector, first_descr_vector, i)\n",
    "            if not output_warp:\n",
    "                return\n",
    "\n",
    "            rand_descr, gradients = output_warp  \n",
    "            for i, g in zip([title, descr, rand_descr], gradients):\n",
    "                update_sgdm(i, g, vt, gamma, alpha)\n",
    "            n_steps += 1\n",
    "            if n_steps%50 == 49:\n",
    "                sys.stdout.write(\"\\rTraining: %f%%\" % (100*(n_steps+1)/len(seq)))\n",
    "                sys.stdout.flush()\n",
    "            if n_steps % check_step == 0:\n",
    "                unitnorm(matrix)\n",
    "        test_recall = recall(test_data)\n",
    "        t = time.process_time() - t1\n",
    "        print(\"Epoch {:>2} : recall = {:>2}%  time ={:>12}s\".format(epoch, round(test_recall, 2), round(t,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# близкие слова\n",
    "\n",
    "def knn(ind, k=10):\n",
    "    dist_arr = {}\n",
    "\n",
    "    for i in range(0, len(matrix)):\n",
    "        dist_arr[i] = get_dist(matrix[ind], matrix[i])\n",
    "\n",
    "    sorted_embs = sorted(dist_arr.items(), key=lambda kv: kv[1])[:k]\n",
    "    for emb in sorted_embs:\n",
    "        if emb[0] != ind:\n",
    "            print(\"\\t\" + dict(zip(vocab.values(), vocab.keys()))[emb[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 99.991736%Epoch  0 : recall = 0.04%  time =     193.578s\n",
      "Training: 99.991736%Epoch  1 : recall = 0.06%  time =     211.266s\n",
      "Training: 99.991736%Epoch  2 : recall = 0.06%  time =     207.828s\n",
      "Training: 99.991736%Epoch  3 : recall = 0.06%  time =     209.312s\n",
      "Training: 99.991736%Epoch  4 : recall = 0.06%  time =     208.703s\n",
      "Training: 99.991736%Epoch  5 : recall = 0.05%  time =     209.516s\n",
      "Training: 99.991736%Epoch  6 : recall = 0.05%  time =     210.547s\n",
      "Training: 99.991736%Epoch  7 : recall = 0.06%  time =     209.625s\n",
      "Training: 99.991736%Epoch  8 : recall = 0.05%  time =     207.719s\n",
      "Training: 99.991736%Epoch  9 : recall = 0.06%  time =     207.547s\n"
     ]
    }
   ],
   "source": [
    "# matrix size = 512\n",
    "matrix, vt = initialize()\n",
    "train(train_data, test_data, 10, vt, 0.95, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tбарселона\n",
      "\tхесе\n",
      "\tдепортиво\n",
      "\tатлетику\n",
      "\tгранады\n",
      "\tкоруньи\n",
      "\tбарселону\n",
      "\tэспаньола\n",
      "\tкаталонцам\n"
     ]
    }
   ],
   "source": [
    "knn(vocab.get(\"реал\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tфорварды\n",
      "\tовечкин\n",
      "\tварламовым\n",
      "\tхоккеистов\n",
      "\tголосование\n",
      "\tхоккей\n",
      "\tнападающие\n",
      "\tмарков\n",
      "\tхоккее\n"
     ]
    }
   ],
   "source": [
    "knn(vocab.get(\"коньки\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
