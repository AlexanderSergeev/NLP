{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0V9MgS0rEg_f",
        "colab_type": "code",
        "outputId": "dba55058-3066-447b-c2e3-a686934a7a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip \"/content/gdrive/My Drive/Colab Notebooks/corpus.zip\"\n",
        "\n",
        "df2 = pd.read_csv('corpus.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Archive:  /content/gdrive/My Drive/Colab Notebooks/corpus.zip\n",
            "  inflating: corpus.csv              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRxSVZY9AT_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df2.drop(columns=['story', 'url', 'descr'])\n",
        "df = df.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMUOxeN-CEqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = df['title'].map(lambda x: len(x)).max()\n",
        "\n",
        "df['y_title'] = df.apply(lambda row: row.title[1:] + \"︱\", axis=1)\n",
        "df['title'] = df.apply(lambda row: row.title + \"〜\" * (max_length-len(row.title)), axis=1)\n",
        "df['y_title'] = df.apply(lambda row: row.y_title + \"〜\" * (max_length-len(row.y_title)), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGw6oRLv8TRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "text=df['title'].str.cat()\n",
        "text_y=df['y_title'].str.cat()\n",
        "\n",
        "chars = tuple(set(text+'︱'))\n",
        "vocab = dict(enumerate(chars))\n",
        "to_int = {ch: ii for ii, ch in vocab.items()}\n",
        "\n",
        "encoded_x = np.array([to_int[ch] for ch in text])\n",
        "encoded_y = np.array([to_int[ch] for ch in text_y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIJC4__48TTZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(arr, n_labels):\n",
        "  one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "  one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "  one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5X3LWGpunB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batches(arr, arr_y, batch_size):\n",
        "  batch_size_total = batch_size * max_length\n",
        "  n_batches = len(arr)//batch_size_total\n",
        "\n",
        "  # Keep only enough characters to make full batches\n",
        "  arr = arr[:n_batches * batch_size_total]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  arr_y = arr_y[:n_batches * batch_size_total]\n",
        "  arr_y = arr_y.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, arr.shape[1], max_length):\n",
        "    x = arr[:, n:n+max_length]\n",
        "    y = arr_y[:, n:n+max_length]\n",
        "    yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMmR86fE8TVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    \n",
        "  def __init__(self, tokens, n_hidden, n_layers, drop_prob, lr):\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_prob\n",
        "    self.n_layers = n_layers\n",
        "    self.n_hidden = n_hidden\n",
        "    self.lr = lr\n",
        "    self.chars = tokens\n",
        "    self.vocab = dict(enumerate(self.chars))\n",
        "    self.to_int = {ch: ii for ii, ch in self.vocab.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "\n",
        "  def forward(self, x, hidden):\n",
        "    #get the outputs and the new hidden state from the lstm\n",
        "    r_output, hidden = self.lstm(x, hidden)\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    out = self.fc(out)\n",
        "    return out, hidden\n",
        "\n",
        "\n",
        "  def hidden_state(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(), weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "    return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJT75DNv8ZtD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, x_data, y_data, epochs, batch_size, max_length, lr, clip, val_percentage, print_every):\n",
        "  net.train()\n",
        "  opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "  loss_obj = nn.CrossEntropyLoss()\n",
        "\n",
        "  val_ids = int(len(x_data)*(1-val_percentage))\n",
        "  x_data, val_x = x_data[:val_ids], x_data[val_ids:]\n",
        "  y_data, val_y = y_data[:val_ids], y_data[val_ids:]\n",
        "\n",
        "  net.cuda()\n",
        "\n",
        "  counter = 0\n",
        "  n_chars = len(net.chars)\n",
        "  for e in range(epochs):\n",
        "    h = net.hidden_state(batch_size)\n",
        "\n",
        "    for x, y in batches(x_data, y_data, batch_size):\n",
        "      counter += 1\n",
        "\n",
        "      x = one_hot(x, n_chars)\n",
        "      inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "      h = tuple([each.data for each in h])\n",
        "      net.zero_grad()\n",
        "      \n",
        "      output, h = net(inputs, h)\n",
        "\n",
        "      loss = loss_obj(output, targets.view(batch_size*max_length).long())\n",
        "      loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      if counter % print_every == 0:\n",
        "        val_h = net.hidden_state(batch_size)\n",
        "        val_losses = []\n",
        "        net.eval()\n",
        "        for x, y in batches(val_x, val_y, batch_size):\n",
        "          x = one_hot(x, n_chars)\n",
        "          x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "          val_h = tuple([each.data for each in val_h])\n",
        "          inputs, targets = x, y\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "          output, val_h = net(inputs, val_h)\n",
        "          val_loss = loss_obj(output, targets.view(batch_size*max_length).long())\n",
        "          val_losses.append(val_loss.item())\n",
        "\n",
        "        net.train()\n",
        "\n",
        "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "              \"Step: {}...\".format(counter),\n",
        "              \"Loss: {:.4f}...\".format(loss.item()),\n",
        "              \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHzVshNe8ZvJ",
        "colab_type": "code",
        "outputId": "8ae940a5-741f-4616-9ff9-d011082afac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2414
        }
      },
      "source": [
        "n_hidden=512\n",
        "n_layers=2\n",
        "\n",
        "net = LSTM(chars, n_hidden, n_layers)\n",
        "print(net)\n",
        "\n",
        "batch_size = 128\n",
        "n_epochs = 20\n",
        "train(net, encoded_x, encoded_y, epochs=n_epochs, batch_size=batch_size, max_length=max_length, lr=0.001, print_every=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(181, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=181, bias=True)\n",
            ")\n",
            "Epoch: 1/20... Step: 100... Loss: 1.9232... Val Loss: 1.3955\n",
            "Epoch: 1/20... Step: 200... Loss: 1.8015... Val Loss: 1.2957\n",
            "Epoch: 1/20... Step: 300... Loss: 1.5018... Val Loss: 1.0855\n",
            "Epoch: 1/20... Step: 400... Loss: 1.3636... Val Loss: 1.0310\n",
            "Epoch: 1/20... Step: 500... Loss: 1.3304... Val Loss: 0.9899\n",
            "Epoch: 1/20... Step: 600... Loss: 1.2822... Val Loss: 0.9522\n",
            "Epoch: 2/20... Step: 700... Loss: 1.2721... Val Loss: 0.9316\n",
            "Epoch: 2/20... Step: 800... Loss: 1.2276... Val Loss: 0.8927\n",
            "Epoch: 2/20... Step: 900... Loss: 1.2046... Val Loss: 0.8655\n",
            "Epoch: 2/20... Step: 1000... Loss: 1.0590... Val Loss: 0.8388\n",
            "Epoch: 2/20... Step: 1100... Loss: 1.1144... Val Loss: 0.8214\n",
            "Epoch: 2/20... Step: 1200... Loss: 1.0601... Val Loss: 0.8016\n",
            "Epoch: 2/20... Step: 1300... Loss: 1.0663... Val Loss: 0.7802\n",
            "Epoch: 3/20... Step: 1400... Loss: 1.0208... Val Loss: 0.7731\n",
            "Epoch: 3/20... Step: 1500... Loss: 0.9959... Val Loss: 0.7556\n",
            "Epoch: 3/20... Step: 1600... Loss: 0.9654... Val Loss: 0.7393\n",
            "Epoch: 3/20... Step: 1700... Loss: 0.9461... Val Loss: 0.7290\n",
            "Epoch: 3/20... Step: 1800... Loss: 0.9594... Val Loss: 0.7200\n",
            "Epoch: 3/20... Step: 1900... Loss: 0.9525... Val Loss: 0.7142\n",
            "Epoch: 3/20... Step: 2000... Loss: 0.9319... Val Loss: 0.6974\n",
            "Epoch: 4/20... Step: 2100... Loss: 0.9058... Val Loss: 0.6901\n",
            "Epoch: 4/20... Step: 2200... Loss: 0.8606... Val Loss: 0.6856\n",
            "Epoch: 4/20... Step: 2300... Loss: 0.8488... Val Loss: 0.6791\n",
            "Epoch: 4/20... Step: 2400... Loss: 0.8597... Val Loss: 0.6704\n",
            "Epoch: 4/20... Step: 2500... Loss: 0.8799... Val Loss: 0.6651\n",
            "Epoch: 4/20... Step: 2600... Loss: 0.8308... Val Loss: 0.6579\n",
            "Epoch: 4/20... Step: 2700... Loss: 0.8362... Val Loss: 0.6462\n",
            "Epoch: 5/20... Step: 2800... Loss: 0.8521... Val Loss: 0.6406\n",
            "Epoch: 5/20... Step: 2900... Loss: 0.7915... Val Loss: 0.6401\n",
            "Epoch: 5/20... Step: 3000... Loss: 0.8025... Val Loss: 0.6357\n",
            "Epoch: 5/20... Step: 3100... Loss: 0.7856... Val Loss: 0.6334\n",
            "Epoch: 5/20... Step: 3200... Loss: 0.7486... Val Loss: 0.6289\n",
            "Epoch: 5/20... Step: 3300... Loss: 0.8037... Val Loss: 0.6197\n",
            "Epoch: 5/20... Step: 3400... Loss: 0.7403... Val Loss: 0.6134\n",
            "Epoch: 6/20... Step: 3500... Loss: 0.7462... Val Loss: 0.6142\n",
            "Epoch: 6/20... Step: 3600... Loss: 0.7620... Val Loss: 0.6087\n",
            "Epoch: 6/20... Step: 3700... Loss: 0.7268... Val Loss: 0.6056\n",
            "Epoch: 6/20... Step: 3800... Loss: 0.7340... Val Loss: 0.6013\n",
            "Epoch: 6/20... Step: 3900... Loss: 0.7141... Val Loss: 0.5973\n",
            "Epoch: 6/20... Step: 4000... Loss: 0.7654... Val Loss: 0.5926\n",
            "Epoch: 7/20... Step: 4100... Loss: 0.7773... Val Loss: 0.5910\n",
            "Epoch: 7/20... Step: 4200... Loss: 0.7255... Val Loss: 0.5915\n",
            "Epoch: 7/20... Step: 4300... Loss: 0.7240... Val Loss: 0.5938\n",
            "Epoch: 7/20... Step: 4400... Loss: 0.6690... Val Loss: 0.5884\n",
            "Epoch: 7/20... Step: 4500... Loss: 0.7121... Val Loss: 0.5843\n",
            "Epoch: 7/20... Step: 4600... Loss: 0.7341... Val Loss: 0.5799\n",
            "Epoch: 7/20... Step: 4700... Loss: 0.7071... Val Loss: 0.5744\n",
            "Epoch: 8/20... Step: 4800... Loss: 0.7009... Val Loss: 0.5708\n",
            "Epoch: 8/20... Step: 4900... Loss: 0.7195... Val Loss: 0.5806\n",
            "Epoch: 8/20... Step: 5000... Loss: 0.6596... Val Loss: 0.5695\n",
            "Epoch: 8/20... Step: 5100... Loss: 0.6377... Val Loss: 0.5686\n",
            "Epoch: 8/20... Step: 5200... Loss: 0.7264... Val Loss: 0.5641\n",
            "Epoch: 8/20... Step: 5300... Loss: 0.6734... Val Loss: 0.5782\n",
            "Epoch: 8/20... Step: 5400... Loss: 0.6844... Val Loss: 0.5666\n",
            "Epoch: 9/20... Step: 5500... Loss: 0.6680... Val Loss: 0.5589\n",
            "Epoch: 9/20... Step: 5600... Loss: 0.6801... Val Loss: 0.5600\n",
            "Epoch: 9/20... Step: 5700... Loss: 0.6607... Val Loss: 0.5544\n",
            "Epoch: 9/20... Step: 5800... Loss: 0.6698... Val Loss: 0.5564\n",
            "Epoch: 9/20... Step: 5900... Loss: 0.6607... Val Loss: 0.5510\n",
            "Epoch: 9/20... Step: 6000... Loss: 0.6479... Val Loss: 0.5540\n",
            "Epoch: 9/20... Step: 6100... Loss: 0.6679... Val Loss: 0.5464\n",
            "Epoch: 10/20... Step: 6200... Loss: 0.6596... Val Loss: 0.5512\n",
            "Epoch: 10/20... Step: 6300... Loss: 0.6823... Val Loss: 0.5517\n",
            "Epoch: 10/20... Step: 6400... Loss: 0.6382... Val Loss: 0.5448\n",
            "Epoch: 10/20... Step: 6500... Loss: 0.6799... Val Loss: 0.5455\n",
            "Epoch: 10/20... Step: 6600... Loss: 0.6589... Val Loss: 0.5556\n",
            "Epoch: 10/20... Step: 6700... Loss: 0.6780... Val Loss: 0.5419\n",
            "Epoch: 10/20... Step: 6800... Loss: 0.6647... Val Loss: 0.5410\n",
            "Epoch: 11/20... Step: 6900... Loss: 0.6621... Val Loss: 0.5394\n",
            "Epoch: 11/20... Step: 7000... Loss: 0.6626... Val Loss: 0.5401\n",
            "Epoch: 11/20... Step: 7100... Loss: 0.6454... Val Loss: 0.5427\n",
            "Epoch: 11/20... Step: 7200... Loss: 0.6664... Val Loss: 0.5350\n",
            "Epoch: 11/20... Step: 7300... Loss: 0.6410... Val Loss: 0.5423\n",
            "Epoch: 11/20... Step: 7400... Loss: 0.6341... Val Loss: 0.5442\n",
            "Epoch: 12/20... Step: 7500... Loss: 0.6181... Val Loss: 0.5300\n",
            "Epoch: 12/20... Step: 7600... Loss: 0.6389... Val Loss: 0.5399\n",
            "Epoch: 12/20... Step: 7700... Loss: 0.6461... Val Loss: 0.5411\n",
            "Epoch: 12/20... Step: 7800... Loss: 0.6302... Val Loss: 0.5327\n",
            "Epoch: 12/20... Step: 7900... Loss: 0.6254... Val Loss: 0.5335\n",
            "Epoch: 12/20... Step: 8000... Loss: 0.6209... Val Loss: 0.5314\n",
            "Epoch: 12/20... Step: 8100... Loss: 0.6235... Val Loss: 0.5321\n",
            "Epoch: 13/20... Step: 8200... Loss: 0.6053... Val Loss: 0.5374\n",
            "Epoch: 13/20... Step: 8300... Loss: 0.5988... Val Loss: 0.5320\n",
            "Epoch: 13/20... Step: 8400... Loss: 0.5968... Val Loss: 0.5332\n",
            "Epoch: 13/20... Step: 8500... Loss: 0.6378... Val Loss: 0.5289\n",
            "Epoch: 13/20... Step: 8600... Loss: 0.6116... Val Loss: 0.5324\n",
            "Epoch: 13/20... Step: 8700... Loss: 0.6066... Val Loss: 0.5297\n",
            "Epoch: 13/20... Step: 8800... Loss: 0.6331... Val Loss: 0.5262\n",
            "Epoch: 14/20... Step: 8900... Loss: 0.6050... Val Loss: 0.5262\n",
            "Epoch: 14/20... Step: 9000... Loss: 0.6255... Val Loss: 0.5268\n",
            "Epoch: 14/20... Step: 9100... Loss: 0.5989... Val Loss: 0.5218\n",
            "Epoch: 14/20... Step: 9200... Loss: 0.5801... Val Loss: 0.5264\n",
            "Epoch: 14/20... Step: 9300... Loss: 0.5998... Val Loss: 0.5249\n",
            "Epoch: 14/20... Step: 9400... Loss: 0.5845... Val Loss: 0.5246\n",
            "Epoch: 14/20... Step: 9500... Loss: 0.6116... Val Loss: 0.5245\n",
            "Epoch: 15/20... Step: 9600... Loss: 0.6234... Val Loss: 0.5249\n",
            "Epoch: 15/20... Step: 9700... Loss: 0.6221... Val Loss: 0.5231\n",
            "Epoch: 15/20... Step: 9800... Loss: 0.5969... Val Loss: 0.5264\n",
            "Epoch: 15/20... Step: 9900... Loss: 0.6102... Val Loss: 0.5286\n",
            "Epoch: 15/20... Step: 10000... Loss: 0.5658... Val Loss: 0.5217\n",
            "Epoch: 15/20... Step: 10100... Loss: 0.6045... Val Loss: 0.5226\n",
            "Epoch: 15/20... Step: 10200... Loss: 0.5672... Val Loss: 0.5237\n",
            "Epoch: 16/20... Step: 10300... Loss: 0.6238... Val Loss: 0.5204\n",
            "Epoch: 16/20... Step: 10400... Loss: 0.6155... Val Loss: 0.5374\n",
            "Epoch: 16/20... Step: 10500... Loss: 0.5723... Val Loss: 0.5358\n",
            "Epoch: 16/20... Step: 10600... Loss: 0.6076... Val Loss: 0.5211\n",
            "Epoch: 16/20... Step: 10700... Loss: 0.5507... Val Loss: 0.5168\n",
            "Epoch: 16/20... Step: 10800... Loss: 0.5895... Val Loss: 0.5141\n",
            "Epoch: 17/20... Step: 10900... Loss: 0.5935... Val Loss: 0.5148\n",
            "Epoch: 17/20... Step: 11000... Loss: 0.6066... Val Loss: 0.5201\n",
            "Epoch: 17/20... Step: 11100... Loss: 0.6046... Val Loss: 0.5160\n",
            "Epoch: 17/20... Step: 11200... Loss: 0.6256... Val Loss: 0.5317\n",
            "Epoch: 17/20... Step: 11300... Loss: 0.5925... Val Loss: 0.5161\n",
            "Epoch: 17/20... Step: 11400... Loss: 0.6057... Val Loss: 0.5210\n",
            "Epoch: 17/20... Step: 11500... Loss: 0.6031... Val Loss: 0.5140\n",
            "Epoch: 18/20... Step: 11600... Loss: 0.5537... Val Loss: 0.5116\n",
            "Epoch: 18/20... Step: 11700... Loss: 0.5753... Val Loss: 0.5178\n",
            "Epoch: 18/20... Step: 11800... Loss: 0.5990... Val Loss: 0.5153\n",
            "Epoch: 18/20... Step: 11900... Loss: 0.5762... Val Loss: 0.5271\n",
            "Epoch: 18/20... Step: 12000... Loss: 0.5668... Val Loss: 0.5125\n",
            "Epoch: 18/20... Step: 12100... Loss: 0.5539... Val Loss: 0.5171\n",
            "Epoch: 18/20... Step: 12200... Loss: 0.6003... Val Loss: 0.5149\n",
            "Epoch: 19/20... Step: 12300... Loss: 0.5889... Val Loss: 0.5165\n",
            "Epoch: 19/20... Step: 12400... Loss: 0.5376... Val Loss: 0.5144\n",
            "Epoch: 19/20... Step: 12500... Loss: 0.5819... Val Loss: 0.5140\n",
            "Epoch: 19/20... Step: 12600... Loss: 0.6067... Val Loss: 0.5105\n",
            "Epoch: 19/20... Step: 12700... Loss: 0.5814... Val Loss: 0.5099\n",
            "Epoch: 19/20... Step: 12800... Loss: 0.5210... Val Loss: 0.5108\n",
            "Epoch: 19/20... Step: 12900... Loss: 0.5712... Val Loss: 0.5104\n",
            "Epoch: 20/20... Step: 13000... Loss: 0.6169... Val Loss: 0.5187\n",
            "Epoch: 20/20... Step: 13100... Loss: 0.5611... Val Loss: 0.5114\n",
            "Epoch: 20/20... Step: 13200... Loss: 0.5522... Val Loss: 0.5109\n",
            "Epoch: 20/20... Step: 13300... Loss: 0.5616... Val Loss: 0.5118\n",
            "Epoch: 20/20... Step: 13400... Loss: 0.5356... Val Loss: 0.5102\n",
            "Epoch: 20/20... Step: 13500... Loss: 0.5698... Val Loss: 0.5057\n",
            "Epoch: 20/20... Step: 13600... Loss: 0.5291... Val Loss: 0.5080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kxw57IF8-QVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, char, h=None):\n",
        "  x = np.array([[net.to_int[char]]])\n",
        "  x = one_hot(x, len(net.chars))\n",
        "  inputs = torch.from_numpy(x)\n",
        "\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  h = tuple([each.data for each in h])\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the character probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "  p = p.cpu()\n",
        "\n",
        "  p, top_ch = p.topk(5)\n",
        "  top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "  # select the next character\n",
        "  p = p.numpy().squeeze()\n",
        "  char = np.random.choice(top_ch, p=p/p.sum())\n",
        "  \n",
        "  return net.vocab[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giEsewgy8TXr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_text(net, start='Судь'):\n",
        "        \n",
        "  net.cuda()\n",
        "  net.eval()\n",
        "\n",
        "  chars = [ch for ch in start]\n",
        "  h = net.hidden_state(1)\n",
        "  for ch in start:\n",
        "    char, h = predict(net, ch, h)\n",
        "\n",
        "  chars.append(char)\n",
        "\n",
        "  while (char!=\"︱\"):\n",
        "    char, h = predict(net, chars[-1], h)\n",
        "    chars.append(char)\n",
        "\n",
        "  return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45DMT6aNPZJV",
        "colab_type": "code",
        "outputId": "6cf6328b-e75c-4d3f-c5b7-f1d638b8ec74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Судь'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Судья и еще 3 причины проблемы с «Барселоной»︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b15uCXvc-TXf",
        "colab_type": "code",
        "outputId": "9abff231-fa44-4e21-f49d-140267b85461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Роналд'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Роналду признан лучшим игроком месяца в НХЛ︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYSZnjCgPfB8",
        "colab_type": "code",
        "outputId": "6895bef3-1fc9-41cf-e333-b977c2f7bb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Месс'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Месси и Карлос Тевес подписал контракт с «Барселоной»︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDcUfXsluVJB",
        "colab_type": "code",
        "outputId": "6a3a223f-2056-475e-bf07-3c46b6e6e187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Барселон'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Барселона» проиграла «Вальядолидо»︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqE2Fi_7uYr_",
        "colab_type": "code",
        "outputId": "e71c110c-95ee-45ae-9ffc-926854b703e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Ф'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Федерер обыграл Давида Феррера и вышел в четвертый круг︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxxBSZpZn_Vo",
        "colab_type": "code",
        "outputId": "a886b702-0d80-47c2-d396-7dbff64ce8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='Хокке'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Хоккейный полуфинал Кубка Стэнли в последнем матче подряд︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqfcUTOYoJWG",
        "colab_type": "code",
        "outputId": "390c076d-dcc4-4871-d016-464d1c7e269d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(make_text(net, start='В'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Вилья Кербер выиграла спринт в Майами︱\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWbfGF12P5LG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}